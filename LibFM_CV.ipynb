{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LibFM - Cross Validation\n",
    "\n",
    "Here, we rewrite the methods we use for the run with libFM in a way compatible with the cross-validation of the rest of the project.\n",
    "\n",
    "Usual imports first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pywFM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and formatting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_load(path):\n",
    "    # 1. Load the DF and format it\n",
    "    df = pd.read_csv(path)\n",
    "    df['User'] = [(ID.split('_')[0])[1:] for ID in df['Id']]\n",
    "    df['Movie'] = [(ID.split('_')[1])[1:] for ID in df['Id']]\n",
    "    parsed_df = df[['User', 'Movie', 'Prediction']].astype(int)\n",
    "    parsed_df[['Id']] = df[['Id']]\n",
    "\n",
    "    # 2. Sort in ascending way for two variables so we can identify it later on.\n",
    "    parsed_df = parsed_df.sort_values(['Movie','User'],ascending=[True,True])\n",
    "    return parsed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def df_to_sparse(df):\n",
    "    \"\"\"\n",
    "        Rewrites our matrix of user movie association in the following format, starting from a 2 column csv file with :\n",
    "        1st column : user id and movie id mixed, 2nd column : rating. The output matrix will take the form \n",
    "        \n",
    "         Users  |     Movies    \n",
    "        A  B  C | TI  NH  SW  ST\n",
    "        [1, 0, 0,  1,  0,  0,  0],\n",
    "        [1, 0, 0,  0,  1,  0,  0],\n",
    "        [1, 0, 0,  0,  0,  1,  0],\n",
    "        [0, 1, 0,  0,  0,  1,  0],\n",
    "        [0, 1, 0,  0,  0,  0,  1],\n",
    "        [0, 0, 1,  1,  0,  0,  0],\n",
    "        [0, 0, 1,  0,  0,  1,  0] \n",
    "        ])\n",
    "        \n",
    "        target = [5, 3, 1, 4, 5, 1, 5]\n",
    "        \n",
    "        @param path : The path of the training/testing data\n",
    "        @oaram p_test : The percentage of elements that should be in the training\n",
    "        @return features_te, target_te : the testing matrix and testing target values\n",
    "        @return features_tr, target_tr : the training matrix and the training target values\n",
    "    \"\"\"\n",
    "    #1. Extracting the info from the input DFv\n",
    "    user_index = np.squeeze(np.array(df['User']-1))\n",
    "    movie_index = np.squeeze(np.array(df['Movie'] + max(user_index)))\n",
    "    ratings = np.squeeze(np.array(df['Prediction']))\n",
    "    \n",
    "    #2.Formatting now the way we need to use libFM\n",
    "    # a. Testing set\n",
    "    col_entries = np.r_[user_index,movie_index]\n",
    "    indices = np.arange(0,len(user_index))\n",
    "    row_entries = np.r_[indices,indices]\n",
    "    entries = np.ones(len(row_entries))\n",
    "    \n",
    "    features = csr_matrix((entries,(row_entries, col_entries)),shape = (len(indices),len(col_entries)))\n",
    "    \n",
    "    return features, ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_to_sparse_split(df, p_test=0.1):\n",
    "    \"\"\"\n",
    "        Rewrites our matrix of user movie association in the following format, starting from a 2 column csv file with :\n",
    "        1st column : user id and movie id mixed, 2nd column : rating. The output matrix will take the form \n",
    "        \n",
    "         Users  |     Movies    \n",
    "        A  B  C | TI  NH  SW  ST\n",
    "        [1, 0, 0,  1,  0,  0,  0],\n",
    "        [1, 0, 0,  0,  1,  0,  0],\n",
    "        [1, 0, 0,  0,  0,  1,  0],\n",
    "        [0, 1, 0,  0,  0,  1,  0],\n",
    "        [0, 1, 0,  0,  0,  0,  1],\n",
    "        [0, 0, 1,  1,  0,  0,  0],\n",
    "        [0, 0, 1,  0,  0,  1,  0] \n",
    "        ])\n",
    "        \n",
    "        target = [5, 3, 1, 4, 5, 1, 5]\n",
    "        \n",
    "        @param path : The path of the training/testing data\n",
    "        @oaram p_test : The percentage of elements that should be in the training\n",
    "        @return features_te, target_te : the testing matrix and testing target values\n",
    "        @return features_tr, target_tr : the training matrix and the training target values\n",
    "    \"\"\"\n",
    "    #1. Extracting the info from the input DFv\n",
    "    user_index = np.squeeze(np.array(df['User']-1))\n",
    "    movie_index = np.squeeze(np.array(df['Movie'] + max(user_index)))\n",
    "    ratings = np.squeeze(np.array(df['Prediction']))\n",
    "    \n",
    "    #2. Test train split    \n",
    "    # We make no permutation if either everything is a train or a test split.\n",
    "    if p_test > 0 and p_test <1:\n",
    "        indices = np.random.permutation(np.arange(0,len(user_index)))\n",
    "    \n",
    "        idx_te = indices[0:int(len(indices)*p_test)]\n",
    "        idx_tr = indices[int(len(indices)*p_test):]\n",
    "    elif p_test == 1:\n",
    "        idx_te = np.arange(0,len(user_index))\n",
    "        idx_tr = []\n",
    "    else:\n",
    "        idx_te = []\n",
    "        idx_tr = np.arange(0,len(user_index))\n",
    "    \n",
    "    #3.Formatting now the way we need to use libFM\n",
    "    # a. Testing set\n",
    "    col_entries_te = np.r_[user_index[idx_te],movie_index[idx_te]]\n",
    "    indices_te = np.arange(0,len(user_index[idx_te]))\n",
    "    row_entries_te = np.r_[indices_te,indices_te]\n",
    "    entries_te = np.ones(len(row_entries_te))\n",
    "    \n",
    "    features_te = csr_matrix((entries_te,(row_entries_te, col_entries_te)),shape = (len(indices_te),len(col_entries_te)))\n",
    "    target_te = ratings[idx_te]\n",
    "    \n",
    "    # b. Training set\n",
    "    col_entries_tr = np.r_[user_index[idx_tr],movie_index[idx_tr]]\n",
    "    indices_tr = np.arange(0,len(user_index[idx_tr]))\n",
    "    row_entries_tr = np.r_[indices_tr,indices_tr]\n",
    "    entries_tr = np.ones(len(row_entries_tr))\n",
    "    \n",
    "    features_tr = csr_matrix((entries_tr,(row_entries_tr, col_entries_tr)),shape = (len(indices_tr),len(col_entries_tr)))\n",
    "    target_tr = ratings[idx_tr]    \n",
    "    \n",
    "    return features_te, target_te, features_tr, target_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_submission(submission_data_path, prediction, out_path):\n",
    "    df = pd.read_csv(submission_data_path)\n",
    "    df['User'] = [(ID.split('_')[0])[1:] for ID in df['Id']]\n",
    "    df['Movie'] = [(ID.split('_')[1])[1:] for ID in df['Id']]\n",
    "    df[['User','Movie']] = df[['User','Movie']].apply(pd.to_numeric)\n",
    "    df = df.sort_values(['Movie','User'],ascending=[True,True])\n",
    "    df['Prediction'] = prediction.astype(int)\n",
    "    \n",
    "    df[['Id','Prediction']].to_csv(out_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running methods (proper run and CV functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_PATH = \"submission.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Alternating Least Squares (ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_ALS(train_df, num_iter=25, std_init = 0.1, rank = 8, r0_reg = 1.5, r1_reg = 2, r2_reg = 2):\n",
    "    \n",
    "    # 1. Defining the model\n",
    "    fm = pywFM.FM(task = 'regression', learning_method='als', num_iter=num_iter, init_stdev = std_init, k2 = rank,\n",
    "             r0_regularization = r0_reg, r1_regularization = r1_reg, r2_regularization = r2_reg)\n",
    "    \n",
    "    # 2. Formatting the data\n",
    "    features_tr, target_tr = df_to_sparse(train_df)\n",
    "    features_te, target_te = df_to_sparse(df_load(TEST_PATH))\n",
    "    \n",
    "    # 3. Running the model\n",
    "    model = fm.run(features_tr, target_tr, features_te, target_te)\n",
    "    \n",
    "    # 4. Outputs\n",
    "    error = model.rlog.rmse[num_iter-1]\n",
    "    pred = model.predictions\n",
    "    \n",
    "    print(\"Error = \",error,\" (for ALS with \", num_iter, \"iterations, std_init =\",std_init, \", k=\",rank, \", r0_reg=\",r0_reg,\n",
    "         \", r1_reg=\",r1_reg,\", r2_reg =\",r2_reg,\")\")\n",
    "    \n",
    "    return pred, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ALS_CV(features_tr, target_tr, features_te, target_test, num_iter, std_init_vec, rank_vec, r0_reg_vec, r1_reg_vec,r2_reg_vec):\n",
    "    #features_tr, target_tr = df_to_sparse(train)\n",
    "    #features_te, target_te = df_to_sparse(test)\n",
    "    \n",
    "    best_error = 1000000;\n",
    "    best_std = 0;\n",
    "    best_rank = 0;\n",
    "    best_r0 = 0;\n",
    "    best_r1 = 0;\n",
    "    best_r2 = 0;\n",
    "    for std_init in std_init_vec:\n",
    "        for rank in rank_vec:\n",
    "            for r0_reg in r0_reg_vec:\n",
    "                for r1_reg in r1_reg_vec:\n",
    "                    for r2_reg in r2_reg_vec:\n",
    "                        fm = pywFM.FM(task = 'regression', learning_method='als', num_iter=num_iter, \n",
    "                                        init_stdev = std_init, k2 = rank, r0_regularization = r0_reg, \n",
    "                                        r1_regularization = r1_reg, r2_regularization = r2_reg)\n",
    "                        model = fm.run(features_tr, target_tr, features_te, target_te)\n",
    "                        error = model.rlog.rmse[num_iter-1]\n",
    "                        \n",
    "                        print(\"Error = \",error,\" (for ALS with \", num_iter, \"iterations, std_init =\",\n",
    "                            std_init, \", k=\",rank, \", r0_reg=\",r0_reg,\n",
    "                             \", r1_reg=\",r1_reg,\", r2_reg =\",r2_reg,\")\")\n",
    "    \n",
    "                        if error < best_error:\n",
    "                            best_error = error; best_std = std_init; best_rank = rank; best_r0 = r0_reg; best_r1 = r1_reg; best_r2 = r2_reg;   \n",
    "    \n",
    "    return best_error, best_std, best_rank, best_r0, best_r1, best_r2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_MCMC(train_df, num_iter=25, std_init = 0.1):\n",
    "    \n",
    "    # 1. Defining the model\n",
    "    fm = pywFM.FM(task='regression', num_iter= num_iter,init_stdev = std_init)\n",
    "    \n",
    "    # 2. Formatting the data\n",
    "    features_tr, target_tr = df_to_sparse(train_df)\n",
    "    features_te, target_te = df_to_sparse(df_load(TEST_PATH))\n",
    "    \n",
    "    # 3. Running the model\n",
    "    model = fm.run(features_tr, target_tr, features_te, target_te)\n",
    "    \n",
    "    # 4. Outputs\n",
    "    error = model.rlog.rmse[num_iter-1]\n",
    "    pred = model.predictions\n",
    "    \n",
    "    print(\"Error = \",error,\" (for ALS with \", num_iter, \"iterations, std_init =\",std_init,\")\")\n",
    "    \n",
    "    return pred, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MCMC_CV(features_tr, target_tr, features_te, target_test, num_iter, std_init_vec):\n",
    "    best_error = 1000000;\n",
    "    best_std = 0;\n",
    "    for std_init in std_init_vec:\n",
    "            fm = pywFM.FM(task='regression', num_iter= num_iter,init_stdev = std_init)\n",
    "            model = fm.run(features_tr, target_tr, features_te, target_te)\n",
    "            error = model.rlog.rmse[num_iter-1]\n",
    "            print(\"Error = \",error,\" (for MCMC with \", num_iter, \"iterations, std_init =\",std_init,\")\")            \n",
    "            if error < best_error:\n",
    "                best_error = error; best_std = std_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred, error =  run_ALS(df_load(\"data_train.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error =  0.984539  (for ALS with  40 iterations, std_init = 0.375 , k= 7 , r0_reg= 0.5 , r1_reg= 15 , r2_reg = 20 )\n",
      "Error =  0.984155  (for ALS with  40 iterations, std_init = 0.375 , k= 7 , r0_reg= 0.5 , r1_reg= 15 , r2_reg = 25 )\n",
      "Error =  0.983993  (for ALS with  40 iterations, std_init = 0.375 , k= 7 , r0_reg= 0.5 , r1_reg= 20 , r2_reg = 20 )\n",
      "Error =  0.984125  (for ALS with  40 iterations, std_init = 0.375 , k= 7 , r0_reg= 0.5 , r1_reg= 20 , r2_reg = 25 )\n",
      "Error =  0.983576  (for ALS with  40 iterations, std_init = 0.43 , k= 7 , r0_reg= 0.5 , r1_reg= 15 , r2_reg = 20 )\n",
      "Error =  0.983401  (for ALS with  40 iterations, std_init = 0.43 , k= 7 , r0_reg= 0.5 , r1_reg= 15 , r2_reg = 25 )\n",
      "Error =  0.984214  (for ALS with  40 iterations, std_init = 0.43 , k= 7 , r0_reg= 0.5 , r1_reg= 20 , r2_reg = 20 )\n",
      "Error =  0.983691  (for ALS with  40 iterations, std_init = 0.43 , k= 7 , r0_reg= 0.5 , r1_reg= 20 , r2_reg = 25 )\n"
     ]
    }
   ],
   "source": [
    "features_te, target_te, features_tr, target_tr =  df_to_sparse_split(df_load(\"data_train.csv\"),0.1)\n",
    "best_error, best_std, best_rank, best_r0, best_r1, best_r2 = ALS_CV(features_tr, target_tr, features_te, target_te,\n",
    "        num_iter = 40, std_init_vec=[0.375,0.43], rank_vec=[7], r0_reg_vec=[0.5], r1_reg_vec=[15,20],r2_reg_vec=[20,25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error =  0.681716  (for ALS with  40 iterations, std_init = 0.43 , k= 7 , r0_reg= 0.5 , r1_reg= 15 , r2_reg = 25 )\n"
     ]
    }
   ],
   "source": [
    "pred, error = run_ALS(df_load(\"data_train.csv\"), num_iter=100, std_init = 0.43, rank = 7, r0_reg = 0.5, r1_reg = 15, r2_reg = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_submission(\"submission.csv\", np.round(pred), \"out_ALS_40_034_7_05_15_25.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.,  3.,  4., ...,  3.,  3.,  4.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error =  0.979054  (for MCMC with  60 iterations, std_init = 0.5 )\n",
      "Error =  0.979325  (for MCMC with  60 iterations, std_init = 0.8 )\n",
      "Error =  0.978956  (for MCMC with  60 iterations, std_init = 1 )\n",
      "Error =  0.979788  (for MCMC with  60 iterations, std_init = 1.5 )\n",
      "Error =  0.986542  (for MCMC with  60 iterations, std_init = 2 )\n",
      "Error =  1.01712  (for MCMC with  60 iterations, std_init = 5 )\n"
     ]
    }
   ],
   "source": [
    "features_te, target_te, features_tr, target_tr =  df_to_sparse_split(df_load(\"data_train.csv\"),0.1)\n",
    "MCMC_CV(features_tr, target_tr, features_te, target_te, num_iter = 60, std_init_vec = [0.5,0.8,1,1.5,2,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "features_te, target_te, features_tr, target_tr =  df_to_sparse_split(df_load(\"data_train.csv\"),0.1)\n",
    "best_error, best_std, best_rank, best_r0, best_r1, best_r2 = ALS_CV(features_tr, target_tr, features_te, target_te, num_iter = 40, std_init_vec=[0.01,0.05,0.1], rank_vec=[7,9], r0_reg_vec=[0.5, 2], r1_reg_vec=[0.5, 2],r2_reg_vec=[0.5, 2])\n",
    "```\n",
    "Error =  0.997084  (for ALS with  40 iterations, std_init = 0.1 , k= 7 , r0_reg= 0.5 , r1_reg= 2 , r2_reg = 2 )\n",
    "\n",
    "\n",
    "```\n",
    "features_te, target_te, features_tr, target_tr =  df_to_sparse_split(df_load(\"data_train.csv\"),0.1)\n",
    "best_error, best_std, best_rank, best_r0, best_r1, best_r2 = ALS_CV(features_tr, target_tr, features_te, target_te,\n",
    "        num_iter = 40, std_init_vec=[0.5,1], rank_vec=[7,8], r0_reg_vec=[0.5, 2], r1_reg_vec=[2,3],r2_reg_vec=[2,3])\n",
    "```\n",
    "\n",
    "Error =  0.993506  (for ALS with  40 iterations, std_init = 0.5 , k= 7 , r0_reg= 0.5 , r1_reg= 3 , r2_reg = 3 )\n",
    "\n",
    "```\n",
    "best_error, best_std, best_rank, best_r0, best_r1, best_r2 = ALS_CV(features_tr, target_tr, features_te, target_te,\n",
    "        num_iter = 40, std_init_vec=[0.375,0.43], rank_vec=[7], r0_reg_vec=[0.5], r1_reg_vec=[15,20],r2_reg_vec=[20,25])\n",
    "```\n",
    "Error =  0.983401  (for ALS with  40 iterations, std_init = 0.43 , k= 7 , r0_reg= 0.5 , r1_reg= 15 , r2_reg = 25 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "features_te, target_te, features_tr, target_tr =  df_to_sparse_split(df_load(\"data_train.csv\"),0.1)\n",
    "MCMC_CV(features_tr, target_tr, features_te, target_te, num_iter = 60, std_init_vec = [0.5,0.8,1,1.5,2,5])\n",
    "```\n",
    "\n",
    "- Error =  0.979054  (for MCMC with  60 iterations, std_init = 0.5 )\n",
    "- Error =  0.979325  (for MCMC with  60 iterations, std_init = 0.8 )\n",
    "- Error =  0.978956  (for MCMC with  60 iterations, std_init = 1 )\n",
    "- Error =  0.979788  (for MCMC with  60 iterations, std_init = 1.5 )\n",
    "- Error =  0.986542  (for MCMC with  60 iterations, std_init = 2 )\n",
    "- Error =  1.01712  (for MCMC with  60 iterations, std_init = 5 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
